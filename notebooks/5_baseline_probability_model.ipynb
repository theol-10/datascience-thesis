{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d3edd9-a911-42c7-bd10-5b1512c33d74",
   "metadata": {},
   "source": [
    "# Baseline Probability Model Based on Historical Frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9dcaa-858e-4c73-be62-be057a86242c",
   "metadata": {},
   "source": [
    "This notebook implements a simple, interpretable baseline model that predicts traffic severity using empirical class probabilities conditioned on the time of day, weekday, and road segment. These probabilities are computed using historical data and serve as a reference point for evaluating more complex models later in the project.\n",
    "\n",
    "Therefore, in this notebook, no machine learning is used.\n",
    "\n",
    "The approach is useful because it reflects recurring traffic patterns and allows for probability-based feature engineering, which is later used to enhance the performance of machine learning models.\n",
    "\n",
    "How the probabilities are calculated:\n",
    "\n",
    "- I'm using empirical (historical) frequency-based probabilities, purely based on counting past events.\n",
    "- I group the historical dataset by: road, hour, weekday\n",
    "- For each group, I count how many times each severity level (0, 1, 2) occurred.\n",
    "- Then, I normalize those counts to get probabilities.\n",
    "  \n",
    "These probabilities are not predictions from a model, but rather statistical summaries from the historical data.\n",
    "\n",
    "This creates a simple, interpretable baseline.\n",
    "\n",
    "These probabilities can later be added as features to ML models (e.g., Random Forest, XGBoost) â€” and they often improve performance, especially in structured time series or location-based tasks like traffic forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0a82cb-c72c-4ac8-bd5f-0290cd1b2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline probabilities saved \n",
      "\n",
      "Accuracy: 0.795\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89     63108\n",
      "           1       0.47      0.03      0.06     11823\n",
      "           2       0.40      0.00      0.00      4557\n",
      "\n",
      "    accuracy                           0.79     79488\n",
      "   macro avg       0.56      0.34      0.32     79488\n",
      "weighted avg       0.73      0.79      0.71     79488\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[62797   304     7]\n",
      " [11434   384     5]\n",
      " [ 4421   128     8]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Load engineered dataset\n",
    "df = pd.read_csv(\"../data/engineered_traffic_data.csv\")\n",
    "\n",
    "# Parse timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Extract hour and weekday for grouping\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.dayofweek  # 0 = Monday\n",
    "\n",
    "# Map status to numeric severity levels\n",
    "severity_map = {'Good': 0, 'Minor': 1, 'Serious': 2}\n",
    "df['severity_level'] = df['status'].map(severity_map)\n",
    "\n",
    "# Create the baseline probability lookup table by\n",
    "# computing the distribution of severity classes for each (road, hour, weekday) combination using normalized counts\n",
    "prob_df = (\n",
    "    df.groupby(['road', 'hour', 'weekday'])['severity_level']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns for clarity \n",
    "prob_df = prob_df.rename(columns={\n",
    "    0: 'prob_severity_0',\n",
    "    1: 'prob_severity_1',\n",
    "    2: 'prob_severity_2'\n",
    "})\n",
    "\n",
    "# Save baseline probabilities for future notebooks\n",
    "prob_df.to_csv(\"../results/baseline_probabilities.csv\", index=False)\n",
    "\n",
    "print(\"Baseline probabilities saved \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EVALUATION\n",
    "# baseline prediction based on most likely class\n",
    "\n",
    "# Create a lookup dictionary for quick predictions\n",
    "lookup = (\n",
    "    prob_df\n",
    "    .set_index(['road', 'hour', 'weekday'])\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "\n",
    "# Function to get prediction based on highest probability\n",
    "def predict_baseline(row):\n",
    "    key = (row['road'], row['hour'], row['weekday'])\n",
    "    probs = lookup.get(key, None)\n",
    "    if probs:\n",
    "        # Pick class with max probability\n",
    "        pred = max(probs, key=probs.get)\n",
    "        return int(pred.split('_')[-1])\n",
    "    else:\n",
    "        return 0  # Default to 'Good' if no data\n",
    "\n",
    "# Apply baseline predictions\n",
    "df['baseline_prediction'] = df.apply(predict_baseline, axis=1)\n",
    "\n",
    "# Evaluate performance\n",
    "y_true = df['severity_level']\n",
    "y_pred = df['baseline_prediction']\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\nAccuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da89ec-bcad-4e6c-8391-5d7d588c69ae",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The baseline model achieves an overall accuracy of 79.5%, primarily due to its strong performance on predicting the 'Good' severity class (Precision: 0.80, Recall: 1.00). However, the model struggles to predict 'Minor' and 'Serious' delays, which are significantly underrepresented in the dataset and exhibit more variability that cannot be fully captured by time-based aggregation alone.\n",
    "\n",
    "As expected, this baseline approach performs reasonably well on the majority class but fails to capture more complex dependencies that may influence the occurrence of traffic incidents. Nevertheless, these estimated probabilities still capture useful temporal patterns, and can be leveraged as additional features in more advanced models to potentially improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3f6be-7c04-430e-92c6-cb37c133a65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
