{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5429dc5-dcba-4885-b22e-01c5290eb211",
   "metadata": {},
   "source": [
    " \n",
    "# Machine Learning Models Based on Recent Traffic History & Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b691d-96f6-489c-b3d2-4cae97c7395c",
   "metadata": {},
   "source": [
    "In this notebook, I train and evaluate the same machine learning models as in Notebook 7a, but now include weather features as additional predictors. The goal is to assess whether incorporating weather data improves prediction performance. We again compare Random Forest, Logistic Regression, and Gradient Boosting models on the extended feature set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7fc0a8-9370-4ffb-b869-1f007d0c3585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     12622\n",
      "           1       0.25      0.07      0.11      2365\n",
      "           2       0.15      0.03      0.05       911\n",
      "\n",
      "    accuracy                           0.77     15898\n",
      "   macro avg       0.40      0.35      0.35     15898\n",
      "weighted avg       0.68      0.77      0.71     15898\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12059   444   119]\n",
      " [ 2144   176    45]\n",
      " [  799    82    30]]\n",
      "Saved model to ../models/7b_random_forest.joblib\n",
      "\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.89     12622\n",
      "           1       0.00      0.00      0.00      2365\n",
      "           2       0.00      0.00      0.00       911\n",
      "\n",
      "    accuracy                           0.79     15898\n",
      "   macro avg       0.26      0.33      0.30     15898\n",
      "weighted avg       0.63      0.79      0.70     15898\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12620     2     0]\n",
      " [ 2365     0     0]\n",
      " [  908     3     0]]\n",
      "Saved model to ../models/7b_logistic_regression.joblib\n",
      "\n",
      "Model: Gradient Boosting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.89     12622\n",
      "           1       0.00      0.00      0.00      2365\n",
      "           2       0.00      0.00      0.00       911\n",
      "\n",
      "    accuracy                           0.79     15898\n",
      "   macro avg       0.26      0.33      0.30     15898\n",
      "weighted avg       0.63      0.79      0.70     15898\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12618     4     0]\n",
      " [ 2365     0     0]\n",
      " [  910     1     0]]\n",
      "Saved model to ../models/7b_gradient_boosting.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather_scaled.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    'prev_1h_severity', 'prev_2h_severity', 'hour', 'day_of_week', 'is_weekend', 'is_rush_hour',\n",
    "    'temperature_2m', 'precipitation', 'rain', 'snowfall', 'wind_speed_10m', 'wind_gusts_10m', 'cloud_cover'\n",
    "]\n",
    "X = df[features]\n",
    "y = df['severity_level']\n",
    "\n",
    "# rain-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate, save each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Save model\n",
    "    filename = f\"../models/7b_{name.lower().replace(' ', '_')}.joblib\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Saved model to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d187c-b087-4921-bcbb-35190702ed6a",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "- Adding weather slightly improved minority class recall (esp. for class 1).\n",
    "- Precision for class 1 increased from 0.19 to 0.25.\n",
    "- But overall still struggles.\n",
    "\n",
    "Logistic Regression & Gradient Boosting:\n",
    "\n",
    "- Exactly same behavior as before — weather features didn’t help at all.\n",
    "- This is because both models underperform heavily with such imbalance and weak signals.\n",
    "- Accuracy dropped slightly (79% to 77%) — which is not really bad... it's expected: now the model tries to correctly classify minority classes instead of just predicting class 0.\n",
    "\n",
    "\n",
    " Random Forest benefits a little from weather features — weak but non-zero signal.\n",
    "Weather data likely introduces a very mild correlation to traffic severity.\n",
    "The class imbalance still dominates.\n",
    "\n",
    "Random Forest is the strongest candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498a01b-809e-45ff-ab27-6b53fd48a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
