{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a508e324-3e75-4928-adc0-941f61870e68",
   "metadata": {},
   "source": [
    "# Random Forest Experiments with Engineered Features & Probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07aa65a-140d-415d-8fdc-2fc5127b69ae",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we develop a hybrid model that integrates both the probabilistic baseline estimates (obtained in Notebook 5) and the engineered features used in previous machine learning models (lagged severity, time features, and weather data). The goal of this approach is to combine long-term temporal patterns (captured by the baseline probabilities) with recent contextual information (captured by lag features and weather), in order to improve the model's ability to predict traffic severity levels.\n",
    "\n",
    "The previously calculated class probabilities are merged into the training dataset and provided as additional input features to a Random Forest classifier, which has shown competitive performance in earlier experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319b1dfc-c736-4c6d-99a1-72a0688d9c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model (RF + Probabilities + Weather + Recent History)\n",
      "\n",
      "Accuracy: 0.7529\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86     18933\n",
      "           1       0.25      0.13      0.17      3547\n",
      "           2       0.15      0.06      0.09      1367\n",
      "\n",
      "    accuracy                           0.75     23847\n",
      "   macro avg       0.41      0.37      0.37     23847\n",
      "weighted avg       0.69      0.75      0.71     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17420  1166   347]\n",
      " [ 2974   449   124]\n",
      " [ 1119   162    86]]\n",
      "Hybrid model saved to ../models/8_hybrid_model.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "#Load engineered data with lags and weather\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather.csv\")\n",
    "\n",
    "# Parse timestamp if not already\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = pd.to_datetime(df['timestamp']).dt.dayofweek\n",
    "\n",
    "\n",
    "# Load baseline probabilities\n",
    "prob_df = pd.read_csv(\"../results/baseline_probabilities.csv\")\n",
    "\n",
    "# Merge probabilities into main dataset\n",
    "df_merged = pd.merge(\n",
    "    df, \n",
    "    prob_df, \n",
    "    on=['road', 'hour', 'weekday'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing probabilities with 0 (\n",
    "df_merged[['prob_severity_0', 'prob_severity_1', 'prob_severity_2']] = df_merged[\n",
    "    ['prob_severity_0', 'prob_severity_1', 'prob_severity_2']\n",
    "].fillna(0)\n",
    "\n",
    "\n",
    "# Prepare final features\n",
    "\n",
    "# Define features to use\n",
    "feature_cols = [\n",
    "    'prev_1h_severity',\n",
    "    'prev_2h_severity',\n",
    "    'temperature_2m',\n",
    "    'precipitation',\n",
    "    'rain',\n",
    "    'snowfall',\n",
    "    'wind_speed_10m',\n",
    "    'wind_gusts_10m',\n",
    "    'cloud_cover',\n",
    "    'prob_severity_0',\n",
    "    'prob_severity_1',\n",
    "    'prob_severity_2'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'severity_level'\n",
    "\n",
    "# Prepare train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_merged[feature_cols],\n",
    "    df_merged[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df_merged[target]\n",
    ")\n",
    "\n",
    "# Train Random Forest model\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#\n",
    "# Evaluate\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Hybrid Model (RF + Probabilities + Weather + Recent History)\")\n",
    "print(\"\\nAccuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, \"../models/8_hybrid_model.joblib\")\n",
    "print(\"Hybrid model saved to ../models/8_hybrid_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3477ad-5b23-4b7d-bb00-6df9ca2307fe",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "- The hybrid model achieves an accuracy of ~75%, comparable to previous models.\n",
    "- It performs well on class 0 (\"Good\") — precision 0.81, recall 0.92.\n",
    "- The model still struggles on minority classes (1=\"Minor\", 2=\"Serious\"), with limited recall and precision — likely due to class imbalance.\n",
    "- Compared to the models in Notebooks 7a and 7b, adding baseline probabilities improves class 1 and 2 slightly (especially recall).\n",
    "- Overall, this model demonstrates that combining prior probabilities and recent history adds some predictive benefit, though minority class prediction remains difficult.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b628322-8cb4-43c2-97cc-4da25146efba",
   "metadata": {},
   "source": [
    "## Class weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3841a9-8ed5-4763-99f4-918e78ab6800",
   "metadata": {},
   "source": [
    "Since the dataset is heavily imbalanced, now I will apply class weighting in the Random Forest classifier, automatically adjusting the penalty for each class inversely proportional to its frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bc305d-1f5a-4ee0-8414-0899af0ef738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model (RF + Probabilities + Weather + Recent History) with Class Weighting\n",
      "\n",
      "Accuracy: 0.7206\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84     18933\n",
      "           1       0.22      0.16      0.18      3547\n",
      "           2       0.12      0.07      0.09      1367\n",
      "\n",
      "    accuracy                           0.72     23847\n",
      "   macro avg       0.38      0.37      0.37     23847\n",
      "weighted avg       0.68      0.72      0.70     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[16527  1842   564]\n",
      " [ 2844   564   139]\n",
      " [ 1075   198    94]]\n",
      "Weighted model saved to ../models/8b_weighted_model.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather.csv\")\n",
    "\n",
    "# Parse timestamp \n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = pd.to_datetime(df['timestamp']).dt.dayofweek\n",
    "\n",
    "# Load baseline probabilities\n",
    "prob_df = pd.read_csv(\"../results/baseline_probabilities.csv\")\n",
    "\n",
    "# Merge probabilities into main dataset\n",
    "df_merged = pd.merge(\n",
    "    df, \n",
    "    prob_df, \n",
    "    on=['road', 'hour', 'weekday'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "prob_df = prob_df.rename(columns={\n",
    "    'p_good': 'prob_severity_0',\n",
    "    'p_minor': 'prob_severity_1',\n",
    "    'p_serious': 'prob_severity_2'\n",
    "})\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df, \n",
    "    prob_df, \n",
    "    on=['road', 'hour', 'weekday'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fillinh missing probabilities with 0 \n",
    "df_merged[['prob_severity_0', 'prob_severity_1', 'prob_severity_2']] = df_merged[\n",
    "    ['prob_severity_0', 'prob_severity_1', 'prob_severity_2']\n",
    "].fillna(0)\n",
    "\n",
    "# Prepare final features\n",
    "\n",
    "# Define features to use\n",
    "feature_cols = [\n",
    "    'prev_1h_severity',\n",
    "    'prev_2h_severity',\n",
    "    'temperature_2m',\n",
    "    'precipitation',\n",
    "    'rain',\n",
    "    'snowfall',\n",
    "    'wind_speed_10m',\n",
    "    'wind_gusts_10m',\n",
    "    'cloud_cover',\n",
    "    'prob_severity_0',\n",
    "    'prob_severity_1',\n",
    "    'prob_severity_2'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'severity_level'\n",
    "\n",
    "# Prepare train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_merged[feature_cols],\n",
    "    df_merged[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df_merged[target]\n",
    ")\n",
    "\n",
    "# Train Random Forest model with class weighting\n",
    "\n",
    "model_weighted = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "model_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "y_pred_weighted = model_weighted.predict(X_test)\n",
    "\n",
    "report_weighted = classification_report(y_test, y_pred_weighted)\n",
    "conf_matrix_weighted = confusion_matrix(y_test, y_pred_weighted)\n",
    "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "\n",
    "print(\"Hybrid Model (RF + Probabilities + Weather + Recent History) with Class Weighting\")\n",
    "print(\"\\nAccuracy:\", round(accuracy_weighted, 4))\n",
    "print(\"\\nClassification Report:\\n\", report_weighted)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_weighted)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model_weighted, \"../models/8b_weighted_model.joblib\")\n",
    "print(\"Weighted model saved to ../models/8b_weighted_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c60c51-696b-4aaa-b416-d66f7582b3d9",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "- Overall accuracy dropped slightly (as expected, since we're forcing model to care more about rare classes).\n",
    "- Recall improved for minority classes 1 and 2 (more correct detections of delays).\n",
    "- Precision for rare classes remains low — not surprising given class imbalance.\n",
    "\n",
    "So it appears that class weighting helps recover more rare cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214fd9f-509d-428d-a4ea-ce31e1c09818",
   "metadata": {},
   "source": [
    "## Additional Time-based Features\n",
    "In this step, we extend the hybrid model by incorporating additional time-based features:\n",
    "\n",
    "- is_weekend: indicates whether the day is Saturday or Sunday.\n",
    "\n",
    "- is_rush_hour: flags typical morning (7–9 AM) and evening (4–6 PM) rush periods.\n",
    "\n",
    "- day_of_week: captures the weekday index (Monday = 0, Sunday = 6).\n",
    "\n",
    "These features aim to help the model capture temporal patterns in traffic behavior which may not be fully captured by hour alone. The model still uses recent traffic history, weather features, and baseline severity probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682c1eac-5eee-4855-8c42-fe9587cca7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model (RF + Probabilities + Weather + Recent History + Time Features)\n",
      "\n",
      "Accuracy: 0.7169\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84     18933\n",
      "           1       0.21      0.16      0.19      3547\n",
      "           2       0.12      0.08      0.09      1367\n",
      "\n",
      "    accuracy                           0.72     23847\n",
      "   macro avg       0.38      0.37      0.37     23847\n",
      "weighted avg       0.68      0.72      0.70     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[16410  1936   587]\n",
      " [ 2800   582   165]\n",
      " [ 1063   201   103]]\n",
      "Model saved to ../models/8c_best_hybrid_model.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather.csv\")\n",
    "\n",
    "# Parse timestamp if not already\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "# Add additional time features\n",
    "df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n",
    "df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)\n",
    "df['day_of_week'] = df['weekday']  # (for modeling)\n",
    "\n",
    "\n",
    "# Load baseline probabilities\n",
    "prob_df = pd.read_csv(\"../results/baseline_probabilities.csv\")\n",
    "\n",
    "# Merge probabilities into main dataset\n",
    "df_merged = pd.merge(\n",
    "    df, \n",
    "    prob_df, \n",
    "    on=['road', 'hour', 'weekday'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename columns if necessary (in case old file format)\n",
    "if 'p_good' in df_merged.columns:\n",
    "    df_merged.rename(columns={'p_good': 'prob_severity_0',\n",
    "                               'p_minor': 'prob_severity_1',\n",
    "                               'p_serious': 'prob_severity_2'}, inplace=True)\n",
    "\n",
    "# Fill missing probabilities conservatively\n",
    "df_merged[['prob_severity_0', 'prob_severity_1', 'prob_severity_2']] = df_merged[\n",
    "    ['prob_severity_0', 'prob_severity_1', 'prob_severity_2']\n",
    "].fillna(0)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Features for model\n",
    "\n",
    "feature_cols = [\n",
    "    'prev_1h_severity',\n",
    "    'prev_2h_severity',\n",
    "    'temperature_2m',\n",
    "    'precipitation',\n",
    "    'rain',\n",
    "    'snowfall',\n",
    "    'wind_speed_10m',\n",
    "    'wind_gusts_10m',\n",
    "    'cloud_cover',\n",
    "    'prob_severity_0',\n",
    "    'prob_severity_1',\n",
    "    'prob_severity_2',\n",
    "    'is_weekend',\n",
    "    'is_rush_hour',\n",
    "    'day_of_week'\n",
    "]\n",
    "\n",
    "target = 'severity_level'\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_merged[feature_cols],\n",
    "    df_merged[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df_merged[target]\n",
    ")\n",
    "\n",
    "# Train model (keeping balanced weights from previous step)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Hybrid Model (RF + Probabilities + Weather + Recent History + Time Features)\")\n",
    "print(\"\\nAccuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "joblib.dump(model, \"../models/8c_best_hybrid_model.joblib\")\n",
    "print(\"Model saved to ../models/8c_best_hybrid_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca504bc-5633-403f-8e89-06f96ca263fd",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "- Accuracy dropped very slightly again (from ~0.72 before to 0.7169 now).\n",
    "\n",
    "- Class 0 (Good) is still predicted quite well.\n",
    "\n",
    "- Minor and Serious Delays (classes 1 and 2) continue to be under-predicted, though recall for class 1 is a bit better than for class 2.\n",
    "\n",
    "- The time features I added (weekend, rush hour, day of week) don’t seem to strongly affect the model — likely because the traffic patterns in London don’t exhibit very sharp peaks in my current dataset, or the historical features already capture most of that variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1162cd7-ac34-4717-8a9f-9abd575623dd",
   "metadata": {},
   "source": [
    "Adding extra time-based features did not lead to significant improvements in overall model accuracy or class balance. This may indicate that temporal effects are already partially captured through the combination of recent history (lag features), weather, and baseline severity probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8c556-078f-45e3-bc29-439a93197b92",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76910f-dd8b-4944-ae86-9361d06d2035",
   "metadata": {},
   "source": [
    "\n",
    "Next, we perform hyperparameter optimization on the Random Forest model using GridSearchCV. The goal is to explore different configurations (number of estimators, tree depth, splitting rules, and leaf sizes) to improve the model's ability to handle class imbalance and improve prediction accuracy across all severity levels. The macro F1 score is used as the optimization metric to give equal weight to each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db308831-e42d-4d5e-bc60-b6e160773d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Random Forest with Hyperparameter Tuning\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Accuracy: 0.5607\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.58      0.71     18933\n",
      "           1       0.26      0.43      0.32      3547\n",
      "           2       0.15      0.65      0.25      1367\n",
      "\n",
      "    accuracy                           0.56     23847\n",
      "   macro avg       0.44      0.55      0.42     23847\n",
      "weighted avg       0.77      0.56      0.62     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10964  4101  3868]\n",
      " [  926  1512  1109]\n",
      " [  186   287   894]]\n",
      "Tuned model saved to ../models/8d_rf_hyperparam_tuned.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/8d_rf_gridsearchcv.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'class_weight': ['balanced']  # keep class weighting active\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',  # we use macro f1 to balance class imbalance\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict with best model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest with Hyperparameter Tuning\")\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"\\nAccuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "joblib.dump(best_rf, \"../models/8d_rf_hyperparam_tuned.joblib\")\n",
    "print(\"Tuned model saved to ../models/8d_rf_hyperparam_tuned.joblib\")\n",
    "\n",
    "# Optional: save the full GridSearchCV object if you want to inspect later\n",
    "joblib.dump(grid_search, \"../models/8d_rf_gridsearchcv.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0d361-3fd8-4157-8943-03cd380e80f4",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "Best parameters found:\n",
    "\n",
    "- n_estimators = 200\n",
    "- max_depth = 10\n",
    "- min_samples_leaf = 2\n",
    "- min_samples_split = 2\n",
    "- class_weight = balanced\n",
    "\n",
    "Accuracy dropped from ~75% (original RF) to 56% — which at first looks worse.\n",
    "\n",
    "BUT:\n",
    "\n",
    "Recall for minority classes (severity 1 and severity 2) significantly increased.\n",
    "\n",
    "For severity 2 (serious):\n",
    "\n",
    "- Recall improved from ~6% → 65%.\n",
    "- F1-score also improved compared to before.\n",
    "\n",
    "The model sacrifices some accuracy on the majority class (severity 0), but gains sensitivity on minority classes.\n",
    "This is a typical trade-off when handling imbalanced classification:\n",
    "\n",
    "Before tuning, the model was overly biased toward predicting the majority class.\n",
    "After tuning, it started identifying minority cases more often — which is valuable for rare but important traffic events.\n",
    "\n",
    "\n",
    "**Conclusion** \n",
    "\n",
    "The drop in accuracy to 56% can still be considered an improvement, depending on the objective.\n",
    "The goal is not just overall accuracy.\n",
    "The problem is imbalanced classification. The vast majority of samples are “Good” (severity 0), so a model can cheat — by always predicting severity 0 — and still get ~80% accuracy.\n",
    "\n",
    "But that kind of model is useless if it completely fails to detect the minority classes (which represent actual traffic disruptions).\n",
    "\n",
    "| Class       | Recall Before | Recall After |\n",
    "| ----------- | ------------- | ------------ |\n",
    "| 0 (Good)    | 92%           | 58%          |\n",
    "| 1 (Minor)   | 13%           | 43%          |\n",
    "| 2 (Serious) | 6%            | 65%          |\n",
    "\n",
    "So while the model is now less confident in always predicting \"Good\", it has become much better at detecting real disruptions.\n",
    "\n",
    "If the objective is realistic traffic forecasting, we'd want the model to actually predict delays, not just \"Good\" status.\n",
    "\n",
    "So this version is more balanced and more useful — even if the accuracy metric alone is lower.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751f2a9-7de4-4c98-ab52-f65e902f7721",
   "metadata": {},
   "source": [
    "### Fine-tuning \n",
    "\n",
    "Next, we try fine-tune the Random Forest model using a soft class weighting approach.\n",
    "Specifically, we use class_weight='balanced_subsample' and limit the model complexity (e.g., max_depth=15) to balance overall accuracy and recall for minority classes.\n",
    "The goal is to retain 65–70% accuracy while improving detection of severity levels 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8417b60f-79cc-435b-986b-53c2b740c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest (Balanced Subsample, Medium Depth)\n",
      "\n",
      "Accuracy: 0.638\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78     18933\n",
      "           1       0.27      0.39      0.32      3547\n",
      "           2       0.16      0.43      0.24      1367\n",
      "\n",
      "    accuracy                           0.64     23847\n",
      "   macro avg       0.44      0.51      0.44     23847\n",
      "weighted avg       0.75      0.64      0.68     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[13249  3401  2283]\n",
      " [ 1456  1383   708]\n",
      " [  402   382   583]]\n",
      "Model saved to ../models/8e_rf_moderate_tuned.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features from hybrid model (same as before)\n",
    "feature_cols = [\n",
    "    'prev_1h_severity',\n",
    "    'prev_2h_severity',\n",
    "    'temperature_2m',\n",
    "    'precipitation',\n",
    "    'rain',\n",
    "    'snowfall',\n",
    "    'wind_speed_10m',\n",
    "    'wind_gusts_10m',\n",
    "    'cloud_cover',\n",
    "    'prob_severity_0',\n",
    "    'prob_severity_1',\n",
    "    'prob_severity_2'\n",
    "]\n",
    "\n",
    "target = 'severity_level'\n",
    "\n",
    "# Re-split in case needed (or reuse X_train, X_test, etc.)\n",
    "X = df_merged[feature_cols]\n",
    "y = df_merged[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train with softer class weighting + moderate depth\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=15,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Tuned Random Forest (Balanced Subsample, Medium Depth)\\n\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "joblib.dump(model, \"../models/8e_rf_moderate_tuned.joblib\")\n",
    "print(\"Model saved to ../models/8e_rf_moderate_tuned.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e711af-517b-4c8a-959e-2b08dade8110",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "Tuned Random Forest Summary (Balanced Subsample + Controlled Complexity)\n",
    " \n",
    "Accuracy: 63.8%\n",
    "(Down from ~75% in the original model, but much more balanced)\n",
    "\n",
    "Recall improvements:\n",
    "\n",
    "- Severity 1: increased to 39% (vs. ~13% before)\n",
    "\n",
    "- Severity 2: increased to 43% (vs. ~6% before)\n",
    "\n",
    "Trade-off:\n",
    "The model sacrifices some overall accuracy (especially for class 0) to better capture minority classes (1 and 2). This leads to more equitable predictions in real-world use cases where detecting delays is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17586e-f1d2-4586-b2a7-fa8198343396",
   "metadata": {},
   "source": [
    "-----\n",
    "### Experiment: Adding Baseline Entropy as a Feature\n",
    "\n",
    "I compute the entropy of the baseline severity probability distribution for each instance, capturing the uncertainty in the baseline estimate. Higher entropy implies more ambiguity. We add this as an extra feature to our existing model and evaluate its contribution to classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d9c478-8ff1-4cc7-a7a0-7bcffac800c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with Baseline Entropy Feature\n",
      "\n",
      "Accuracy: 0.7508\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86     18933\n",
      "           1       0.25      0.13      0.17      3547\n",
      "           2       0.14      0.06      0.08      1367\n",
      "\n",
      "    accuracy                           0.75     23847\n",
      "   macro avg       0.40      0.37      0.37     23847\n",
      "weighted avg       0.69      0.75      0.71     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17372  1205   356]\n",
      " [ 2965   452   130]\n",
      " [ 1126   160    81]]\n",
      "Model with entropy saved to ../models/8f_rf_with_entropy.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load engineered traffic data with weather features\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather.csv\")\n",
    "\n",
    "# Load baseline probabilities (using correct column names)\n",
    "prob_df = pd.read_csv(\"../results/baseline_probabilities.csv\")\n",
    "prob_df = prob_df.rename(columns={\n",
    "    'prob_severity_0': 'p_good',\n",
    "    'prob_severity_1': 'p_minor',\n",
    "    'prob_severity_2': 'p_serious'\n",
    "})\n",
    "\n",
    "# Ensure timestamp-related columns exist\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Merge probabilities into main dataset\n",
    "df_merged = pd.merge(df, prob_df, on=['road', 'hour', 'weekday'], how='left')\n",
    "df_merged[['p_good', 'p_minor', 'p_serious']] = df_merged[\n",
    "    ['p_good', 'p_minor', 'p_serious']\n",
    "].fillna(0)\n",
    "\n",
    "# Compute entropy of the probability distribution\n",
    "df_merged['baseline_entropy'] = entropy(\n",
    "    df_merged[['p_good', 'p_minor', 'p_serious']].values.T,\n",
    "    base=2\n",
    ")\n",
    "\n",
    "# Define features\n",
    "feature_cols = [\n",
    "    'prev_1h_severity', 'prev_2h_severity',\n",
    "    'temperature_2m', 'precipitation', 'rain', 'snowfall',\n",
    "    'wind_speed_10m', 'wind_gusts_10m', 'cloud_cover',\n",
    "    'p_good', 'p_minor', 'p_serious',\n",
    "    'baseline_entropy'\n",
    "]\n",
    "target = 'severity_level'\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_merged[feature_cols],\n",
    "    df_merged[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df_merged[target]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model with Baseline Entropy Feature\")\n",
    "print(\"\\nAccuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "joblib.dump(model, \"../models/8f_rf_with_entropy.joblib\")\n",
    "print(\"Model with entropy saved to ../models/8f_rf_with_entropy.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d98285-67ca-4865-9957-d84b6365e02a",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "- Overall accuracy stayed roughly the same (from ~0.7529 before to 0.7508 now).\n",
    "\n",
    "- Recall for classes 1 and 2 slightly improved (compared to the original baseline), even if marginally.\n",
    "\n",
    "- Entropy provided a tiny boost in class balance awareness, but not dramatically — likely because the Random Forest already leverages decision uncertainty fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0bd13c-7df0-45f9-8fc2-0f11d7468621",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26547a-85ea-4abe-aa02-3a87f3802359",
   "metadata": {},
   "source": [
    "### Combining all of the above into one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705f46f0-458e-45cb-920f-bee9a4e09e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model: All Features + Tuning + Entropy\n",
      "\n",
      "Accuracy: 0.5587\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.58      0.71     18933\n",
      "           1       0.26      0.42      0.32      3547\n",
      "           2       0.15      0.66      0.24      1367\n",
      "\n",
      "    accuracy                           0.56     23847\n",
      "   macro avg       0.44      0.55      0.42     23847\n",
      "weighted avg       0.77      0.56      0.62     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10931  4058  3944]\n",
      " [  920  1494  1133]\n",
      " [  182   286   899]]\n",
      "Final model saved to ../models/8g_final_model_with_entropy.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load main dataset\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "# Load and rename baseline probabilities\n",
    "baseline_probs = pd.read_csv(\"../results/baseline_probabilities.csv\")\n",
    "baseline_probs = baseline_probs.rename(columns={\n",
    "    'prob_severity_0': 'p_good',\n",
    "    'prob_severity_1': 'p_minor',\n",
    "    'prob_severity_2': 'p_serious'\n",
    "})\n",
    "\n",
    "# Merge probabilities\n",
    "df = pd.merge(df, baseline_probs, on=['road', 'hour', 'weekday'], how='left')\n",
    "df[['p_good', 'p_minor', 'p_serious']] = df[['p_good', 'p_minor', 'p_serious']].fillna(0)\n",
    "\n",
    "# Calculate entropy\n",
    "df['entropy'] = df[['p_good', 'p_minor', 'p_serious']].apply(lambda row: entropy(row + 1e-9, base=2), axis=1)\n",
    "\n",
    "# Final feature list\n",
    "features = [\n",
    "    'prev_1h_severity', 'prev_2h_severity',\n",
    "    'temperature_2m', 'precipitation', 'rain', 'snowfall',\n",
    "    'wind_speed_10m', 'wind_gusts_10m', 'cloud_cover',\n",
    "    'p_good', 'p_minor', 'p_serious',\n",
    "    'entropy'\n",
    "]\n",
    "X = df[features]\n",
    "y = df['severity_level']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Tuned Random Forest\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=2,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Final Model: All Features + Tuning + Entropy\")\n",
    "print(\"\\nAccuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "joblib.dump(model, \"../models/8g_final_model_with_entropy.joblib\")\n",
    "print(\"Final model saved to ../models/8g_final_model_with_entropy.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf827330-241e-4b0d-8e26-918aec1bc920",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333a6db-180e-4b89-857c-37ccc5bd7d8b",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "| Model Variant                                      | Accuracy  | Recall (0) | Recall (1) | Recall (2) | Macro F1 | Notes                                  |\n",
    "| -------------------------------------------------- | --------- | ---------- | ---------- | ---------- | -------- | -------------------------------------- |\n",
    "| **Baseline Probability Only**                      | 0.795     | **1.00**   | 0.03       | 0.00       | 0.32     | Predicts mostly class 0                |\n",
    "| **Recent History + Weather (RF)**                  | 0.7529    | 0.92       | 0.13       | 0.06       | 0.37     | Balanced, simple RF                    |\n",
    "| **+ Class Weighting**                              | 0.7206    | 0.87       | 0.16       | 0.07       | 0.37     | Slight boost to class 1/2              |\n",
    "| **+ Hyperparameter Tuning (Full Grid Search)**     | 0.5587    | 0.58       | 0.42       | **0.66**   | 0.42     | Boosts minority recall, hurts accuracy |\n",
    "| ** Tuned RF (Balanced Subsample, Medium Depth)** | **0.638** | 0.70       | 0.39       | 0.43       | **0.44** | Best trade-off                       |\n",
    "| **+ Entropy Feature**                              | 0.7508    | 0.92       | 0.13       | 0.06       | 0.37     | Didn’t improve much over base RF       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df0155-95e7-46e8-a0f3-9670dc89ff2c",
   "metadata": {},
   "source": [
    "The tuned RF seems to be the best choice, having a good balance, so I will now include entropy in that model to see if we have a further improvement.\n",
    "\n",
    "### Incorporating entropy in Tuned RF\n",
    "\n",
    "This experiment adds a new feature representing the entropy (uncertainty) of baseline class probabilities. It aims to help the model better judge when to rely more on its own prediction vs. when the baseline gives strong signals (i.e., low entropy = confident prior, high entropy = uncertain). We include it on top of the tuned Random Forest with class balancing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d35cab-c6c7-4c07-95d4-83716c9c61fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with Entropy + All Previous Improvements\n",
      "\n",
      "Accuracy: 0.5587\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.58      0.71     18933\n",
      "           1       0.26      0.42      0.32      3547\n",
      "           2       0.15      0.66      0.24      1367\n",
      "\n",
      "    accuracy                           0.56     23847\n",
      "   macro avg       0.44      0.55      0.42     23847\n",
      "weighted avg       0.77      0.56      0.62     23847\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10931  4058  3944]\n",
      " [  920  1494  1133]\n",
      " [  182   286   899]]\n",
      "Saved: 8h_model_with_entropy_and_all_improvements.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "# Load baseline probabilities\n",
    "prob_df = pd.read_csv(\"../results/baseline_probabilities.csv\")\n",
    "\n",
    "# Rename columns for consistency\n",
    "prob_df = prob_df.rename(columns={\n",
    "    'p_good': 'prob_severity_0',\n",
    "    'p_minor': 'prob_severity_1',\n",
    "    'p_serious': 'prob_severity_2'\n",
    "})\n",
    "\n",
    "# Merge baseline probabilities\n",
    "df = pd.merge(df, prob_df, on=['road', 'hour', 'weekday'], how='left')\n",
    "df[['prob_severity_0', 'prob_severity_1', 'prob_severity_2']] = df[[\n",
    "    'prob_severity_0', 'prob_severity_1', 'prob_severity_2'\n",
    "]].fillna(0)\n",
    "\n",
    "# Compute entropy of probabilities\n",
    "def compute_entropy(row):\n",
    "    probs = np.array([row['prob_severity_0'], row['prob_severity_1'], row['prob_severity_2']])\n",
    "    probs = np.clip(probs, 1e-9, 1)  # avoid log(0)\n",
    "    return -np.sum(probs * np.log(probs))\n",
    "\n",
    "df['prob_entropy'] = df.apply(compute_entropy, axis=1)\n",
    "\n",
    "# Feature columns\n",
    "feature_cols = [\n",
    "    'prev_1h_severity', 'prev_2h_severity',\n",
    "    'temperature_2m', 'precipitation', 'rain', 'snowfall',\n",
    "    'wind_speed_10m', 'wind_gusts_10m', 'cloud_cover',\n",
    "    'prob_severity_0', 'prob_severity_1', 'prob_severity_2',\n",
    "    'prob_entropy'\n",
    "]\n",
    "\n",
    "# Target\n",
    "target = 'severity_level'\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[feature_cols], df[target],\n",
    "    test_size=0.3, random_state=42, stratify=df[target]\n",
    ")\n",
    "\n",
    "# Train tuned RF with entropy\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=2,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model with Entropy + All Previous Improvements\\n\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "joblib.dump(model, \"../models/8h_model_with_entropy_and_all_improvements.joblib\")\n",
    "print(\"Saved: 8h_model_with_entropy_and_all_improvements.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9d776-c347-4dc4-84fb-be27c873f97d",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "- The overall accuracy dropped to ~56%, and Class 0 (Good): Very high precision, but low recall (~58%)\n",
    "\n",
    "- Classes 1 & 2 (Minor/Serious): Recall improved, but precision remains low\n",
    "\n",
    "- The model is now more willing to predict classes 1 and 2, which improves recall, but at the cost of many false positives for those classes.\n",
    "\n",
    "This model is useful if the goal is to catch more minor/serious cases (better recall for classes 1–2), even at the expense of many false alarms.\n",
    "\n",
    "For a balanced performance, the tuned RF (without entropy) is the best choice (and best overall candidate):\n",
    "- Tuned Random Forest (Balanced Subsample, Medium Depth)\n",
    "- Accuracy: 0.638, better balance of precision/recall across classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e575b-85e7-486f-8951-a6f9118b5a9a",
   "metadata": {},
   "source": [
    "Entropy is a valid idea conceptually (especially in a hybrid model), but it didn't help in this case, likely due to overlap with existing features and misleading signals from uncertain baseline distributions.\n",
    "\n",
    "If included, it should ideally be paired with strong regularization or feature selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a234e-2aba-40cb-83da-f39dcb41233e",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Adding Feature Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06096e2a-d426-4c5d-81da-d076d65272f7",
   "metadata": {},
   "source": [
    "I will now experiment by adding Feature Interactions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea868851-4bda-4700-961b-3bfceff964e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with Feature Interactions (Best Config)\n",
      "\n",
      "Accuracy: 0.5544\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.57      0.70     18933\n",
      "           1       0.25      0.42      0.32      3547\n",
      "           2       0.15      0.67      0.24      1367\n",
      "\n",
      "    accuracy                           0.55     23847\n",
      "   macro avg       0.44      0.55      0.42     23847\n",
      "weighted avg       0.77      0.55      0.62     23847\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10815  4108  4010]\n",
      " [  888  1496  1163]\n",
      " [  176   281   910]]\n",
      "Saved: 8i_rf_feature_interactions.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Load main dataset\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags_and_weather.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "# Load probabilities with correct column names\n",
    "prob_df = pd.read_csv(\"../results/baseline_probabilities.csv\")\n",
    "# Check column names\n",
    "# print(prob_df.columns)\n",
    "\n",
    "# Merge using the correct names\n",
    "df_merged = pd.merge(df, prob_df, on=[\"road\", \"hour\", \"weekday\"], how=\"left\")\n",
    "\n",
    "# Fill missing values\n",
    "df_merged[['prob_severity_0', 'prob_severity_1', 'prob_severity_2']] = df_merged[\n",
    "    ['prob_severity_0', 'prob_severity_1', 'prob_severity_2']\n",
    "].fillna(0)\n",
    "\n",
    "# Add interaction features\n",
    "df_merged['precip_x_cloud'] = df_merged['precipitation'] * df_merged['cloud_cover']\n",
    "df_merged['temp_x_wind'] = df_merged['temperature_2m'] * df_merged['wind_speed_10m']\n",
    "df_merged['rushhour_x_prev1h'] = df_merged['is_rush_hour'] * df_merged['prev_1h_severity']\n",
    "\n",
    "# Feature list\n",
    "feature_cols = [\n",
    "    'prev_1h_severity', 'prev_2h_severity',\n",
    "    'temperature_2m', 'precipitation', 'rain', 'snowfall',\n",
    "    'wind_speed_10m', 'wind_gusts_10m', 'cloud_cover',\n",
    "    'prob_severity_0', 'prob_severity_1', 'prob_severity_2',\n",
    "    'precip_x_cloud', 'temp_x_wind', 'rushhour_x_prev1h'\n",
    "]\n",
    "target = 'severity_level'\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_merged[feature_cols], df_merged[target],\n",
    "    test_size=0.3, random_state=42, stratify=df_merged[target]\n",
    ")\n",
    "\n",
    "#Tuned model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=2,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model with Feature Interactions (Best Config)\")\n",
    "print(\"\\nAccuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "joblib.dump(model, \"../models/8i_rf_feature_interactions.joblib\")\n",
    "print(\"Saved: 8i_rf_feature_interactions.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c190e0-0b13-4668-986d-4076b2a0e373",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "I tested adding the interaction terms: \n",
    "\n",
    "- precip_x_cloud = precipitation × cloud_cover\n",
    "- temp_x_wind = temperature × wind_speed\n",
    "- rushhour_x_prev1h = is_rush_hour × prev_1h_severity\n",
    "\n",
    "They did not improve the model's performance. In fact, results were very similar — slightly worse than the previous best (without interactions)\n",
    "\n",
    "The drop in overall accuracy suggests the added features might be introducing noise or redundancy, rather than new signal.\n",
    "\n",
    "Therefore I'll leave out the interaction features, as they don’t provide measurable benefit here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd21995-3abc-4a72-a448-7c19b4d374ad",
   "metadata": {},
   "source": [
    "----\n",
    "### Best Model Summary:\n",
    "\n",
    "Model type: Random Forest Classifier\n",
    "\n",
    "Key strategies included:\n",
    "\n",
    "- Lag Features (Recent History): prev_1h_severity, prev_2h_severity\n",
    "\n",
    "  Captures short-term temporal patterns in traffic severity per road\n",
    "\n",
    "- Weather Features (Standardized): temperature_2m, precipitation, rain, snowfall, wind_speed_10m, wind_gusts_10m, cloud_cover\n",
    "\n",
    "  External conditions affecting traffic.\n",
    "\n",
    "- Baseline Probability Features: p_good, p_minor, p_serious\n",
    "\n",
    "  Historical severity probabilities by road, hour, and weekday.\n",
    "\n",
    "- Class Weighting: class_weight='balanced_subsample'\n",
    "\n",
    "  Addresses imbalance by giving more weight to underrepresented classes (1 and 2).\n",
    "\n",
    "- Hyperparameter Tuning: max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200\n",
    "\n",
    "  Tuned to improve generalization and performance across classes.\n",
    "\n",
    "\n",
    "**Strong overall accuracy (~0.64)**\n",
    "\n",
    "**Most balanced performance across classes (especially classes 1 & 2)**\n",
    "\n",
    "**Good trade-off between precision and recall**\n",
    "\n",
    "**Reasonable complexity and fast training/inference time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d2638-5238-4ea3-8a86-43f1d53c499d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
