{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39820f6-57d9-48d5-bd40-54ea75c1ae49",
   "metadata": {},
   "source": [
    "# Machine Learning Models Based on Recent Traffic History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fcbaa5-5a15-44eb-b282-373eec99e83b",
   "metadata": {},
   "source": [
    "In this notebook, we train and evaluate multiple machine learning models using historical traffic data only. The dataset includes engineered lag features (previous severity levels at 1h and 2h intervals) as predictors. \n",
    "\n",
    "We compare the performance of three models — Random Forest, Logistic Regression, and Gradient Boosting — to establish a baseline for how well traffic severity can be predicted using only recent history. I picked 3 diverse models, i.e. random forest that is tree-based, logistic regresion that is linear and gradient boosting (ensemble boosting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa451f04-b0b1-43f8-ae20-8e96b5447d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88     12622\n",
      "           1       0.19      0.01      0.01      2365\n",
      "           2       0.23      0.01      0.01       911\n",
      "\n",
      "    accuracy                           0.79     15898\n",
      "   macro avg       0.40      0.34      0.30     15898\n",
      "weighted avg       0.67      0.79      0.70     15898\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12550    58    14]\n",
      " [ 2346    16     3]\n",
      " [  896    10     5]]\n",
      "Saved model to ../models/7a_random_forest.joblib\n",
      "\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.89     12622\n",
      "           1       0.00      0.00      0.00      2365\n",
      "           2       0.00      0.00      0.00       911\n",
      "\n",
      "    accuracy                           0.79     15898\n",
      "   macro avg       0.26      0.33      0.30     15898\n",
      "weighted avg       0.63      0.79      0.70     15898\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12622     0     0]\n",
      " [ 2365     0     0]\n",
      " [  911     0     0]]\n",
      "Saved model to ../models/7a_logistic_regression.joblib\n",
      "\n",
      "Model: Gradient Boosting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.89     12622\n",
      "           1       0.00      0.00      0.00      2365\n",
      "           2       0.00      0.00      0.00       911\n",
      "\n",
      "    accuracy                           0.79     15898\n",
      "   macro avg       0.26      0.33      0.30     15898\n",
      "weighted avg       0.63      0.79      0.70     15898\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12622     0     0]\n",
      " [ 2365     0     0]\n",
      " [  911     0     0]]\n",
      "Saved model to ../models/7a_gradient_boosting.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"../data/engineered_traffic_with_lags.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = ['prev_1h_severity', 'prev_2h_severity', 'hour', 'day_of_week', 'is_weekend', 'is_rush_hour']\n",
    "X = df[features]\n",
    "y = df['severity_level']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "#  Train , evaluate and save each model\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Save model\n",
    "    filename = f\"../models/7a_{name.lower().replace(' ', '_')}.joblib\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Saved model to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30ce15-bbb9-42ec-b7ab-9368b6c159da",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- All models reach high accuracy (79%) — but this is misleading because almost all samples are class 0.\n",
    "Class imbalance dominates the results.\n",
    "\n",
    "- Random Forest at least predicts a few minority class samples (class 1 & 2), but still it's very weak (it seems it's a bit more robust under inbalance).\n",
    "\n",
    "- Logistic Regression and Gradient Boosting entirely collapse: they never predict anything except class 0. It is probably because, inn this notebook, the models are only using recent history features (lags, etc). These features may not carry enough signal to distinguish class 1 and 2.\n",
    "\n",
    "- These results highlight the limitations of using only recent history and motivate the need to incorporate more informative features in subsequent modeling efforts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074644a0-997a-4b07-b07e-d5f58db35fdc",
   "metadata": {},
   "source": [
    "### Note \n",
    "\n",
    "These models offer a more realistic baseline than those in Notebook 3, they show the predictive value of temporal context over structural features. However, of course, there's still a lot to be added to make them more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84749c36-e0b7-4498-b674-220f5bc2fa36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
